# ========================================
# PROMETHEUS STACK - DEV ENVIRONMENT
# ========================================
# Official Chart: kube-prometheus-stack v65.2.0
# Source: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
# Full values reference: See default-values-reference.yaml (5413 lines)
# Documentation: https://prometheus.io/docs/
#
# This file contains CUSTOM VALUES for Dev environment.
# Only overrides necessary settings from chart defaults.
# For all available options, see: default-values-reference.yaml
#
# kube-prometheus-stack includes:
# - Prometheus Server (metrics collection & storage)
# - Grafana (visualization dashboards)
# - AlertManager (alerting)
# - Node Exporter (host metrics)
# - Kube State Metrics (K8s object metrics)
# - Prometheus Operator (CRD management)
#
# References:
# - Prometheus: https://prometheus.io/
# - Grafana: https://grafana.com/
# - AlertManager: https://prometheus.io/docs/alerting/latest/alertmanager/

# ========================================
# PROMETHEUS SERVER
# ========================================
prometheus:
  prometheusSpec:
    # Retention & Storage
    retention: 15d  # Dev: 15 days retention (more history for analysis)
    retentionSize: "30GB"  # Increased for longer retention

    # Resources - Reduced for 2-node cluster
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi

    # Storage
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi  # Reduced for dev
          storageClassName: gp2  # AWS EBS GP2

    # Scrape interval
    scrapeInterval: 30s  # 30s is good balance
    evaluationInterval: 30s

    # Service Monitor selectors
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false

    # Replicas
    replicas: 1  # Single replica for dev

    # Pod anti-affinity (spread across nodes)
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - prometheus
            topologyKey: kubernetes.io/hostname

# ========================================
# GRAFANA
# ========================================
grafana:
  enabled: true

  # Admin credentials
  adminPassword: "admin123"  # CHANGE IN PRODUCTION!

  # Resources - Reduced for dev
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 300m
      memory: 512Mi

  # Persistence
  persistence:
    enabled: true
    size: 10Gi  # More space for dashboards & plugins
    storageClassName: gp2

  # Replicas for HA
  replicas: 1  # Single replica for dev to save resources

  # Pod anti-affinity (spread across nodes)
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: grafana
          topologyKey: kubernetes.io/hostname

  # Service type
  service:
    type: ClusterIP  # Dev: Use kubectl port-forward
    port: 80

  # Ingress disabled for dev (use port-forward)
  ingress:
    enabled: false

  # Default dashboards
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: Asia/Singapore

  # Grafana config
  grafana.ini:
    server:
      root_url: "http://localhost:3000"
    analytics:
      check_for_updates: false
    security:
      allow_embedding: true
    # Performance tuning
    database:
      cache_mode: shared
    plugins:
      enable_alpha: false

# ========================================
# ALERTMANAGER
# ========================================
alertmanager:
  enabled: true

  alertmanagerSpec:
    # Resources - Adequate for alert handling
    resources:
      requests:
        cpu: 100m      # Increased for alert processing
        memory: 256Mi  # More memory for alert history
      limits:
        cpu: 200m      # Allow burst
        memory: 512Mi  # Handle alert storms

    # Storage
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi  # More space for alert history
          storageClassName: gp2

    replicas: 1  # Single replica for dev to save resources

  # Alert configuration (placeholder)
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'null'
    receivers:
      - name: 'null'

# ========================================
# NODE EXPORTER
# ========================================
# Collects hardware and OS metrics from nodes
# Runs as DaemonSet (1 pod per node = 2 pods)
nodeExporter:
  enabled: true

  resources:
    requests:
      cpu: 100m      # Increased for more detailed metrics
      memory: 128Mi  # More memory for metric collection
    limits:
      cpu: 200m      # Allow burst during collection
      memory: 256Mi  # Handle metric spikes

# ========================================
# KUBE STATE METRICS
# ========================================
# Exposes Kubernetes object metrics (pods, deployments, etc)
# More objects = more resources needed
kubeStateMetrics:
  enabled: true

  resources:
    requests:
      cpu: 100m      # Increased for more K8s objects
      memory: 256Mi  # More memory for object tracking
    limits:
      cpu: 250m      # Allow burst
      memory: 512Mi  # Handle large clusters

# ========================================
# PROMETHEUS OPERATOR
# ========================================
prometheusOperator:
  enabled: true

  resources:
    requests:
      cpu: 150m      # Increased for CRD management
      memory: 256Mi  # More memory for operator logic
    limits:
      cpu: 300m      # Allow burst for reconciliation
      memory: 512Mi  # Handle large configurations

  # Admission webhooks
  admissionWebhooks:
    enabled: true
    patch:
      enabled: true
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 100m
          memory: 128Mi

# ========================================
# DEFAULT SERVICE MONITORS
# ========================================
# Disable some default monitors for dev (reduce noise)
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: false  # Not exposed in EKS
    configReloaders: true
    general: true
    k8s: true
    kubeApiserverAvailability: true
    kubeApiserverSlos: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeScheduler: false  # Not exposed in EKS
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true

# ========================================
# ADDITIONAL SCRAPE CONFIGS
# ========================================
# Add custom scrape configs for Flowise (merged into main prometheus config above)
# prometheus:
#   prometheusSpec:
#     additionalScrapeConfigs:
#       - job_name: 'flowise-server'
#         scrape_interval: 30s
#         metrics_path: /api/v1/metrics
#         scheme: http
#         static_configs:
#           - targets: ['flowise-server.flowise-dev.svc.cluster.local:3000']
#             labels:
#               app: 'flowise'
#               component: 'server'
#               environment: 'dev'
